{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "df7b1aef",
   "metadata": {},
   "source": [
    "### Jupyter Notebook Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e34b5b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML                                    \n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))  \n",
    "import IPython.display as display"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12f253c4",
   "metadata": {},
   "source": [
    "### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd2699ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import heapq\n",
    "import struct\n",
    "from tqdm import tqdm\n",
    "from collections import Counter, defaultdict\n",
    "from threading import Lock\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee2e7809",
   "metadata": {},
   "source": [
    "### Conver BPE Vocabulary to a Binary Radix Tree\n",
    "##### It will apply the special characters escaping function to have it working properly in C++\n",
    "##### This function will keep the original BPE \"_\" marker for prefix and \"</w>\" for end of word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c49744f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to escape special characters in a string for C++ string literals\n",
    "def escape_special_characters(token):\n",
    "    token = token.replace('\\\\', '\\\\\\\\')\n",
    "    token = token.replace('\\\"', '\\\\\\\"')\n",
    "    token = token.replace('\\'', '\\\\\\'')\n",
    "    token = token.replace('\\n', '\\\\n')\n",
    "    token = token.replace('\\r', '\\\\r')\n",
    "    token = token.replace('\\t', '\\\\t')\n",
    "    token = token.replace('<space>', ' ')\n",
    "    return token\n",
    "\n",
    "class RadixNode:\n",
    "    def __init__(self):\n",
    "        self.children = {}\n",
    "        self.tokenID = -1\n",
    "\n",
    "class RadixTree:\n",
    "    def __init__(self):\n",
    "        self.root = RadixNode()\n",
    "\n",
    "    def insert(self, word, tokenID):\n",
    "        node = self.root\n",
    "\n",
    "        # Handle words that start with '_'\n",
    "        if word.startswith('_'):\n",
    "            prefix_marker = '_'\n",
    "            word = word[1:]  # Remove the underscore for further processing\n",
    "            if prefix_marker not in node.children:\n",
    "                node.children[prefix_marker] = RadixNode()\n",
    "            node = node.children[prefix_marker]\n",
    "\n",
    "        while word:\n",
    "            for prefix in node.children:\n",
    "                common_length = 0\n",
    "                for i in range(min(len(prefix), len(word))):\n",
    "                    if prefix[i] != word[i]:\n",
    "                        break\n",
    "                    common_length += 1\n",
    "\n",
    "                if common_length > 0:\n",
    "                    if common_length < len(prefix):\n",
    "                        existing_child = node.children.pop(prefix)\n",
    "                        new_node = RadixNode()\n",
    "                        new_node.children[prefix[common_length:]] = existing_child\n",
    "                        node.children[prefix[:common_length]] = new_node\n",
    "                        node = new_node\n",
    "                    else:\n",
    "                        node = node.children[prefix]\n",
    "\n",
    "                    word = word[common_length:]\n",
    "                    break\n",
    "            else:\n",
    "                node.children[word] = RadixNode()\n",
    "                node = node.children[word]\n",
    "                word = \"\"\n",
    "\n",
    "        # Set tokenID at the end of the word\n",
    "        node.tokenID = tokenID\n",
    "\n",
    "    def traverse(self, word):\n",
    "        node = self.root\n",
    "\n",
    "        # Handle words that start with '_'\n",
    "        if word.startswith('_'):\n",
    "            prefix_marker = '_'\n",
    "            word = word[1:]  # Remove the underscore for further processing\n",
    "            if prefix_marker in node.children:\n",
    "                node = node.children[prefix_marker]\n",
    "            else:\n",
    "                return None\n",
    "\n",
    "        while word:\n",
    "            found = False\n",
    "            for prefix, child in node.children.items():\n",
    "                if word.startswith(prefix):\n",
    "                    word = word[len(prefix):]\n",
    "                    node = child\n",
    "                    found = True\n",
    "                    break\n",
    "            if not found:\n",
    "                return None\n",
    "        return node.tokenID if node.tokenID != -1 else None\n",
    "\n",
    "    def serialize_to_binary(self):\n",
    "        def node_to_binary(node):\n",
    "            children_data = b''.join(\n",
    "                struct.pack(f'i{len(prefix)}s', len(prefix), prefix.encode('utf-8')) + node_to_binary(child)\n",
    "                for prefix, child in node.children.items()\n",
    "            )\n",
    "            tokenID_data = struct.pack('i', node.tokenID)\n",
    "            num_children = len(node.children)\n",
    "            num_children_data = struct.pack('i', num_children)\n",
    "            return tokenID_data + num_children_data + children_data\n",
    "\n",
    "        return node_to_binary(self.root)\n",
    "\n",
    "    def deserialize_from_binary(self, binary_data):\n",
    "        def binary_to_node(data, offset=0):\n",
    "            tokenID = struct.unpack_from('i', data, offset)[0]\n",
    "            offset += 4\n",
    "            num_children = struct.unpack_from('i', data, offset)[0]\n",
    "            offset += 4\n",
    "            node = RadixNode()\n",
    "            node.tokenID = tokenID\n",
    "            for _ in range(num_children):\n",
    "                prefix_len = struct.unpack_from('i', data, offset)[0]\n",
    "                offset += 4\n",
    "                prefix = struct.unpack_from(f'{prefix_len}s', data, offset)[0].decode('utf-8')\n",
    "                offset += prefix_len\n",
    "                child, offset = binary_to_node(data, offset)\n",
    "                node.children[prefix] = child\n",
    "            return node, offset\n",
    "\n",
    "        self.root, _ = binary_to_node(binary_data)\n",
    "\n",
    "    def save_to_file(self, filename):\n",
    "        binary_data = self.serialize_to_binary()\n",
    "        with open(filename, 'wb') as file:\n",
    "            file.write(binary_data)\n",
    "\n",
    "    def load_from_file(self, filename):\n",
    "        with open(filename, 'rb') as file:\n",
    "            binary_data = file.read()\n",
    "        self.deserialize_from_binary(binary_data)\n",
    "\n",
    "def load_bpe_vocab(filename):\n",
    "    with open(filename, 'r') as file:\n",
    "        vocab = file.read().splitlines()\n",
    "    return vocab\n",
    "\n",
    "def clean_and_insert_vocab_into_tree(vocab, radix_tree):\n",
    "    for idx, word in enumerate(tqdm(vocab, desc=\"Inserting vocab into Radix Tree\")):\n",
    "        # Process words according to the rules\n",
    "        cleaned_word = escape_special_characters(word)\n",
    "        radix_tree.insert(cleaned_word, idx)\n",
    "\n",
    "def test_radix_tree(radix_tree, test_words):\n",
    "    results = {}\n",
    "    for word in test_words:\n",
    "        cleaned_word = escape_special_characters(word)\n",
    "        result = radix_tree.traverse(cleaned_word)\n",
    "        results[word] = {\"found\": result is not None, \"node\": result}\n",
    "    return results\n",
    "\n",
    "# Load the vocab from the file\n",
    "filename = 'bpe-vocab_bookcorpus-30p_25000-merges_cleaned.txt'\n",
    "vocab = load_bpe_vocab(filename)\n",
    "\n",
    "# Create the Radix Tree\n",
    "radix_tree = RadixTree()\n",
    "\n",
    "# Clean and insert the vocab into the Radix Tree\n",
    "clean_and_insert_vocab_into_tree(vocab, radix_tree)\n",
    "\n",
    "# Save the Radix tree to a .bin file\n",
    "radix_tree.save_to_file('bpe-vocab_bookcorpus-30p_25000-merges_cleaned_radix-tree.bin')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39a27b9b",
   "metadata": {},
   "source": [
    "### Visaulise Radix Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c45567ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_radix_tree(node, prefix='', depth=0, max_depth=3):\n",
    "    \"\"\"\n",
    "    Visualizes part of the Radix Tree.\n",
    "    \n",
    "    Args:\n",
    "    - node: The current RadixNode to visualize.\n",
    "    - prefix: The current prefix string.\n",
    "    - depth: The current depth in the tree.\n",
    "    - max_depth: The maximum depth to traverse for visualization.\n",
    "    \"\"\"\n",
    "    if depth > max_depth:\n",
    "        return\n",
    "\n",
    "    indent = '  ' * depth\n",
    "    node_info = f\"TokenID: {node.tokenID}\" if node.tokenID != -1 else \"No TokenID\"\n",
    "    print(f\"{indent}{prefix} ({node_info})\")\n",
    "\n",
    "    for child_prefix, child_node in node.children.items():\n",
    "        visualize_radix_tree(child_node, prefix + child_prefix, depth + 1, max_depth)\n",
    "\n",
    "# Example usage to visualize the first few levels of the Radix Tree\n",
    "radix_tree_deserialized = RadixTree()\n",
    "radix_tree_deserialized.load_from_file('bpe-vocab_bookcorpus-30p_25000-merges_cleaned_radix-tree.bin')\n",
    "\n",
    "# Visualize the first 3 levels of the Radix Tree starting from the root\n",
    "visualize_radix_tree(radix_tree_deserialized.root, max_depth=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4f3a980",
   "metadata": {},
   "source": [
    "### Convert the binary file to a C header (.h)\n",
    "#### This can also be achieved by using the command `xxd -i bpe_vocab_small_radix_tree.bin > bpe_vocab_small_radix_tree.h` in the terminal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c3c2bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_to_c_header(binary_file_path, header_file_path, array_name):\n",
    "    with open(binary_file_path, \"rb\") as binary_file, open(header_file_path, \"w\") as header_file:\n",
    "        binary_data = binary_file.read()\n",
    "\n",
    "        # Start writing the header file, but we won't count this part in the binary comparison\n",
    "        header_file.write(f\"#ifndef {array_name.upper()}_H\\n\")\n",
    "        header_file.write(f\"#define {array_name.upper()}_H\\n\\n\")\n",
    "        header_file.write(f\"const unsigned char {array_name}[] = {{\\n    \")\n",
    "\n",
    "        # Write the binary data as a C array\n",
    "        for i, byte in enumerate(binary_data):\n",
    "            if i > 0:\n",
    "                header_file.write(\", \")\n",
    "            header_file.write(f\"0x{byte:02x}\")\n",
    "\n",
    "# Regenerate the C header file\n",
    "binary_file_path = \"simplified_radix_tree.bin\"\n",
    "header_file_path = \"simplified_radix_tree.h\"\n",
    "array_name = \"simplified_radix_tree\"\n",
    "\n",
    "binary_to_c_header(binary_file_path, header_file_path, array_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5beed76",
   "metadata": {},
   "source": [
    "### Testing the Binary Tree to find given words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc8dcf9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Radix tree from the .bin file\n",
    "radix_tree_deserialized = RadixTree()\n",
    "radix_tree_deserialized.load_from_file('bpe-vocab_bookcorpus-30p_25000-merges_cleaned_radix-tree.bin')\n",
    "\n",
    "# Test with 5 words from the vocab\n",
    "test_words = [\"rocket</w>\", \"rocketman</w>\", \"exasperated</w>\", \"hello</w>\", \"_am</w>\", \" \", \".\"]\n",
    "results = test_radix_tree(radix_tree_deserialized, test_words)\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3a1d9ba",
   "metadata": {},
   "source": [
    "### Testing the Binary Radix Tree to encode and decode a given phrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b3529c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the BPE vocabulary from the cleaned vocabulary file\n",
    "def load_bpe_vocabulary(vocab_file_path):\n",
    "    with open(vocab_file_path, \"r\") as file:\n",
    "        vocabulary = [line.strip() for line in file.readlines()]\n",
    "    # Remove the `</w>` suffix and handle the <space> token\n",
    "    vocabulary = [token[:-4] if token.endswith(\"</w>\") else token for token in vocabulary]\n",
    "    vocab_dict = {token: idx for idx, token in enumerate(vocabulary)}\n",
    "    return vocab_dict, vocabulary\n",
    "\n",
    "# Function to clean the input: remove non-ASCII characters, trim whitespace, and convert to lowercase\n",
    "def clean_input(phrase):\n",
    "    # Remove non-ASCII characters and convert to lowercase\n",
    "    cleaned_phrase = ''.join([c.lower() for c in phrase if 32 <= ord(c) <= 126])\n",
    "\n",
    "    # Trim leading and trailing whitespace\n",
    "    cleaned_phrase = cleaned_phrase.strip()\n",
    "\n",
    "    return cleaned_phrase\n",
    "\n",
    "# Tokenize a phrase using the BPE vocabulary\n",
    "def tokenize_bpe(phrase, vocab_dict):\n",
    "    tokens = []\n",
    "    phrase = phrase.replace(\" \", \"<space>\")  # Replace spaces with the <space> token\n",
    "    i = 0\n",
    "    while i < len(phrase):\n",
    "        match = None\n",
    "        max_length = 0\n",
    "        for j in range(i + 1, len(phrase) + 1):\n",
    "            subword = phrase[i:j]\n",
    "            if subword in vocab_dict:\n",
    "                match = subword\n",
    "                max_length = j - i  # Track the length of the longest match\n",
    "\n",
    "        if match:\n",
    "            tokens.append(vocab_dict[match])\n",
    "            i += max_length  # Move the index forward by the length of the matched token\n",
    "        else:\n",
    "            # Debugging: Print the unmatched character(s)\n",
    "            print(f\"No match for: {phrase[i]}\")\n",
    "            i += 1  # Skip characters not found in the vocabulary\n",
    "    return tokens\n",
    "\n",
    "# Convert token IDs to binary representation (24-bit)\n",
    "def tokens_to_binary(tokens):\n",
    "    binary_output = bytearray()\n",
    "    for token_id in tokens:\n",
    "        binary_output.extend(struct.pack('>I', token_id)[1:])  # Pack as big-endian, skip the first byte for 24-bit\n",
    "    return binary_output\n",
    "\n",
    "# Decode the binary output back to the original phrase\n",
    "def decode_bpe(binary_output, vocabulary):\n",
    "    tokens = []\n",
    "    for i in range(0, len(binary_output), 3):\n",
    "        token_id = struct.unpack('>I', b'\\x00' + binary_output[i:i+3])[0]  # Unpack with padding for 24-bit\n",
    "        tokens.append(vocabulary[token_id])\n",
    "    decoded_phrase = ''.join(tokens)\n",
    "    return decoded_phrase.replace(\"<space>\", \" \")  # Replace <space> with a regular space\n",
    "\n",
    "# Load the vocabulary\n",
    "vocab_file_path = \"bpe_vocab_cleaned.txt\"\n",
    "vocab_dict, vocabulary = load_bpe_vocabulary(vocab_file_path)\n",
    "\n",
    "# Input phrase from user\n",
    "phrase = input(\"Enter a phrase to tokenize: \")\n",
    "\n",
    "# Clean the input string\n",
    "cleaned_phrase = clean_input(phrase)\n",
    "\n",
    "# Tokenize the phrase\n",
    "tokens = tokenize_bpe(cleaned_phrase, vocab_dict)\n",
    "print(\"Token IDs:\", tokens)\n",
    "\n",
    "# Convert tokens to binary\n",
    "if tokens:\n",
    "    binary_output = tokens_to_binary(tokens)\n",
    "    print(\"Binary output:\", ' '.join(format(byte, '08b') for byte in binary_output))\n",
    "else:\n",
    "    print(\"No tokens matched.\")\n",
    "\n",
    "# Decode the binary output back to the original phrase\n",
    "if tokens:\n",
    "    decoded_phrase = decode_bpe(binary_output, vocabulary)\n",
    "    print(\"Decoded phrase:\", decoded_phrase)\n",
    "else:\n",
    "    print(\"Nothing to decode.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11 (tf-gpu-m1)",
   "language": "python",
   "name": "tf-gpu-m1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
