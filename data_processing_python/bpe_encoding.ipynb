{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4910c42b",
   "metadata": {},
   "source": [
    "### Jupyter Notebook Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8ab1fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML                                    \n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))  \n",
    "import IPython.display as display"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfb30f11",
   "metadata": {},
   "source": [
    "### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f4c5f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import heapq\n",
    "from tqdm import tqdm\n",
    "from datasets import load_dataset\n",
    "from collections import Counter, defaultdict\n",
    "import threading\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fccf627c",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9eb8371",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "dataset = load_dataset(\"bookcorpus/bookcorpus\", split='train[:30%]')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3590fb85",
   "metadata": {},
   "source": [
    "### Create BPE Vocabulary from given corpus/dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3076e507",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize counters and locks\n",
    "word_counter = Counter()\n",
    "counter_lock = Lock()\n",
    "\n",
    "# Tokenize and count word frequencies\n",
    "def process_document_bpe(document):\n",
    "    local_counter = Counter()\n",
    "    tokens = re.findall(r'\\w+|\\S', document.lower())\n",
    "    local_counter.update(tokens)\n",
    "    \n",
    "    # Safely update the global word_counter\n",
    "    with counter_lock:\n",
    "        word_counter.update(local_counter)\n",
    "\n",
    "# Process the dataset with multithreading\n",
    "with ThreadPoolExecutor(max_workers=8) as executor:  # 8 changes the number of workers\n",
    "    futures = [executor.submit(process_document_bpe, document) for document in dataset['text']]\n",
    "    for future in tqdm(as_completed(futures), total=len(futures), desc=\"Processing dataset\"):\n",
    "        pass\n",
    "\n",
    "# Convert words to character-level with a special end-of-word token (e.g., '##')\n",
    "def get_initial_vocab(word_counter):\n",
    "    vocab = {}\n",
    "    for word in word_counter:\n",
    "        word = ' '.join(list(word)) + ' </w>'\n",
    "        vocab[word] = word_counter[word]\n",
    "    return vocab\n",
    "\n",
    "vocab = get_initial_vocab(word_counter)\n",
    "\n",
    "# BPE algorithm: merging the most frequent pairs\n",
    "def get_pair_frequencies(vocab):\n",
    "    pairs = defaultdict(int)\n",
    "    for word, freq in vocab.items():\n",
    "        symbols = word.split()\n",
    "        for i in range(len(symbols) - 1):\n",
    "            pairs[(symbols[i], symbols[i + 1])] += freq\n",
    "    return pairs\n",
    "\n",
    "def merge_vocab(pair, vocab):\n",
    "    merged_vocab = {}\n",
    "    bigram = ' '.join(pair)\n",
    "    replacement = ''.join(pair)\n",
    "    for word in vocab:\n",
    "        new_word = word.replace(bigram, replacement)\n",
    "        merged_vocab[new_word] = vocab[word]\n",
    "    return merged_vocab\n",
    "\n",
    "num_merges = 100000  # Define how many merges to perform (controls vocababulary size)\n",
    "for i in tqdm(range(num_merges), desc=\"Performing BPE merges\"):\n",
    "    pairs = get_pair_frequencies(vocab)\n",
    "    if not pairs:\n",
    "        break\n",
    "    best_pair = max(pairs, key=pairs.get)\n",
    "    vocab = merge_vocab(best_pair, vocab)\n",
    "\n",
    "# Extract the final vocabulary\n",
    "bpe_vocab = sorted(set(''.join(word.split()) for word in vocab.keys()))\n",
    "\n",
    "# Save the BPE vocabulary to a txt file\n",
    "with open(\"bpe-vocab_bookcorpus-30p_100000-merges.txt\", \"w\") as vocab_file:\n",
    "    for token in bpe_vocab:\n",
    "        vocab_file.write(f\"{token}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4e11af9",
   "metadata": {},
   "source": [
    "### Clean BPE Vocabulary\n",
    "##### This will use a set of rules to clean the generated tokens and reduce the final size (only circa 5% reduction)\n",
    "##### Do note it will remove all the existing symbols or numbers (if they exist) and then readd them at the end of the file, this is done for consistency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "597c055b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a lock for thread-safe operations\n",
    "lock = threading.Lock()\n",
    "\n",
    "def is_valid_token(token, token_freq):\n",
    "    # Expanded whitelist including common short words\n",
    "    whitelist = {\n",
    "        \"a\", \"i\",\"the\", \"an\", \"he\", \"him\", \"it\", \"in\", \"on\", \"at\", \"of\", \"to\", \"by\", \"is\", \"as\", \"up\", \"we\", \"us\", \"me\"\n",
    "    }\n",
    "\n",
    "    # Ignore </w> at the end of the token\n",
    "    if token.endswith('</w>'):\n",
    "        cleaned_token = token[:-4]\n",
    "    else:\n",
    "        cleaned_token = token\n",
    "\n",
    "    cleaned_token = re.sub(r'^\\W+|\\W+$', '', cleaned_token)  # Remove leading/trailing symbols (not all, for example \"_\" is kept as it's important for BPE)\n",
    "\n",
    "    # If the cleaned token matches any in the whitelist, return True immediately\n",
    "    if cleaned_token in whitelist:\n",
    "        return True\n",
    "    \n",
    "    # Remove tokens that consist entirely of symbols based on length\n",
    "    if re.match(r'^[\\W_]+$', cleaned_token):\n",
    "        if cleaned_token[0] == '_':\n",
    "            if len(cleaned_token) > 2:\n",
    "                return False\n",
    "        else:\n",
    "            if len(cleaned_token) > 2:\n",
    "                return False\n",
    "\n",
    "    # Remove any token that has 3 of the same letter repeated consecutively\n",
    "    if re.search(r'(.)\\1\\1', cleaned_token):\n",
    "        return False\n",
    "\n",
    "    # Remove any token that contains a mix of numbers and letters\n",
    "    if re.search(r'\\d', cleaned_token) and re.search(r'[a-zA-Z]', cleaned_token):\n",
    "        return False\n",
    "\n",
    "    # Remove tokens that are too long (20 characters)\n",
    "    if len(cleaned_token) > 20:\n",
    "        return False\n",
    "\n",
    "    # Remove tokens with unusual repeated characters\n",
    "    if any(cleaned_token.count(c) > 3 for c in set(cleaned_token)):\n",
    "        return False\n",
    "\n",
    "    # Remove rare short tokens (single characters not in the whitelist)\n",
    "    if len(cleaned_token) < 3 and token_freq.get(cleaned_token, 0) < 3:  # Adjust frequency threshold here\n",
    "        return False\n",
    "\n",
    "    # Remove tokens that are weird combinations of numbers and letters\n",
    "    if re.match(r'\\d+[a-zA-Z]+$', cleaned_token):\n",
    "        return False\n",
    "\n",
    "    return True\n",
    "\n",
    "def process_token(token, token_freq, processed_vocab):\n",
    "    if is_valid_token(token, token_freq):\n",
    "        with lock:  # Ensure thread safety when modifying shared resources\n",
    "            processed_vocab.append(token)\n",
    "\n",
    "# Load the vocabulary from file\n",
    "with open(\"/Users/ciprian/Desktop/Projects/Microcontroller Tokenizer/microcontroller-tokenizer/data/bpe-vocab_bookcorpus-30p_25000-merges.txt\", \"r\") as vocab_file:\n",
    "    bpe_vocab = [line.strip() for line in vocab_file.readlines()]\n",
    "\n",
    "# Assume token frequency data (this can also be had from the tokenization process above)\n",
    "token_freq = Counter(bpe_vocab)  \n",
    "processed_vocab = []\n",
    "\n",
    "# Use ThreadPoolExecutor for multithreading\n",
    "with ThreadPoolExecutor() as executor:\n",
    "    # tqdm for a progress bar\n",
    "    list(tqdm(executor.map(lambda token: process_token(token, token_freq, processed_vocab), bpe_vocab), total=len(bpe_vocab)))\n",
    "\n",
    "# Remove duplicates\n",
    "cleaned_vocab = sorted(set(processed_vocab))\n",
    "\n",
    "# Ensure the digits 0-9, symbols and space tokens are included in the vocabulary\n",
    "space_token = [\"<space>\"]\n",
    "digits = [\"0</w>\", \"1</w>\", \"2</w>\", \"3</w>\", \"4</w>\", \"5</w>\", \"6</w>\", \"7</w>\", \"8</w>\", \"9</w>\"]\n",
    "symbols = [\n",
    "    \".\", \",\", \";\", \":\", \"'\", \"\\\"\", \"!\", \"?\", \"(\", \")\", \"[\", \"]\", \"{\", \"}\",\n",
    "    \"<\", \">\", \"/\", \"\\\\\", \"|\", \"-\", \"_\", \"+\", \"=\", \"*\", \"&\", \"^\", \"%\", \"$\",\n",
    "    \"#\", \"@\", \"~\", \"`\"\n",
    "]\n",
    "\n",
    "cleaned_vocab.extend(space_token)\n",
    "cleaned_vocab.extend(symbols)\n",
    "cleaned_vocab.extend(digits)\n",
    "\n",
    "# Save the cleaned vocabulary back to a file\n",
    "with open(\"/Users/ciprian/Desktop/Projects/Microcontroller Tokenizer/microcontroller-tokenizer/data/bpe-vocab_bookcorpus-30p_25000-merges_cleaned_v2.txt\", \"w\") as cleaned_file:\n",
    "    for token in cleaned_vocab:\n",
    "        cleaned_file.write(f\"{token}\\n\")\n",
    "\n",
    "print(f\"Original vocabulary size: {len(bpe_vocab)}\")\n",
    "print(f\"Cleaned vocabulary size: {len(cleaned_vocab)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68f4a0ec",
   "metadata": {},
   "source": [
    "### Conversion of binary string to phrase (deserialization/reversion of the encoding process)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdd271a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to convert a binary string to bytearray\n",
    "def binary_string_to_bytearray(binary_string):\n",
    "    # Split the input string into binary segments\n",
    "    binary_segments = binary_string.split()\n",
    "\n",
    "    # Ensure each segment is 8 bits long by adding leading zeros if necessary\n",
    "    byte_segments = [segment.zfill(8) for segment in binary_segments]\n",
    "\n",
    "    # Convert each 8-bit segment into a byte and form a bytearray\n",
    "    byte_array = bytearray(int(segment, 2) for segment in byte_segments)\n",
    "\n",
    "    return byte_array\n",
    "\n",
    "# Load the vocabulary\n",
    "vocab_file_path = \"bpe_vocab_cleaned.txt\"\n",
    "vocab_dict, vocabulary = load_bpe_vocabulary(vocab_file_path)\n",
    "\n",
    "# Input string from user, which could be in either format\n",
    "input_binary_string = input(\"Enter the binary string to decode: \")\n",
    "\n",
    "# Convert the input binary string to a bytearray\n",
    "byte_array = binary_string_to_bytearray(input_binary_string)\n",
    "\n",
    "# Decode the bytearray back to the original phrase\n",
    "decoded_phrase = decode_bpe(byte_array, vocabulary)\n",
    "\n",
    "# Output the decoded phrase\n",
    "print(\"Decoded phrase:\", decoded_phrase)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11 (tf-gpu-m1)",
   "language": "python",
   "name": "tf-gpu-m1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
