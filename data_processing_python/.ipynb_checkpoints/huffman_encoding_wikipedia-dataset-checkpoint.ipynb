{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c0125a2f",
   "metadata": {},
   "source": [
    "### Jupyter Notebook Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8fe1dab4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ry/454yhlzx6hd15j7rjv4th0lw0000gn/T/ipykernel_2771/2003764878.py:1: DeprecationWarning: Importing display from IPython.core.display is deprecated since IPython 7.14, please import from IPython display\n",
      "  from IPython.core.display import display, HTML\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML                                    \n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))  \n",
    "import IPython.display as display"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d5bb20a",
   "metadata": {},
   "source": [
    "### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d47443dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet\n",
    "import nltk\n",
    "import string\n",
    "import heapq\n",
    "from collections import Counter\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from tqdm import tqdm\n",
    "from threading import Lock\n",
    "from datasets import load_dataset\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77929f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load necessary NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "wordnet.ensure_loaded()   # Ensure WordNet is fully loaded before starting any threads"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47de4145",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd5590f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset \n",
    "dataset = load_dataset(\"bookcorpus/bookcorpus\", split='train')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "293a1318",
   "metadata": {},
   "source": [
    "### Build the Huffman Tree as binary\n",
    "##### The reason for binary encoding is to save memory on the microcontroller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45337003",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize lemmatizer and counters\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "word_counter = Counter()\n",
    "suffix_counter = Counter()\n",
    "punctuation_counter = Counter()\n",
    "\n",
    "# Define common suffixes and punctuation symbols\n",
    "suffixes = [\"ing\", \"ed\", \"es\", \"s\", \"ly\", \"er\", \"ion\", \"al\", \"able\", \"ness\", \"ful\", \"less\", \"ous\", \"ment\", \"ive\", \"ize\", \"en\", \"ity\", \"ant\"]\n",
    "punctuation_symbols = [\" \", \".\", \",\", \"!\", \"?\", \";\", \":\", \"-\", \"_\", \"(\", \")\", \"[\", \"]\", \"{\", \"}\", \"\\\"\", \"'\", \"â€¦\", \"@\", \"#\", \"$\", \"%\", \"^\", \"&\", \"*\", \"+\", \"=\", \"<\", \">\", \"/\", \"\\\\\", \"|\", \"~\", \"`\"]\n",
    "\n",
    "# Lemmatization cache to speed up processing\n",
    "lemmatization_cache = {}\n",
    "\n",
    "# Locks for thread safety\n",
    "counter_lock = Lock()\n",
    "lemmatization_cache_lock = Lock()\n",
    "\n",
    "def cached_lemmatize(word, lemmatizer):\n",
    "    with lemmatization_cache_lock:\n",
    "        if word in lemmatization_cache:\n",
    "            return lemmatization_cache[word]\n",
    "        else:\n",
    "            lemmatized_word = lemmatizer.lemmatize(word)\n",
    "            lemmatization_cache[word] = lemmatized_word\n",
    "            return lemmatized_word\n",
    "\n",
    "def process_document(document):\n",
    "    local_word_counter = Counter()\n",
    "    local_suffix_counter = Counter()\n",
    "    local_punctuation_counter = Counter()\n",
    "\n",
    "    tokens = nltk.word_tokenize(document.lower())\n",
    "    for token in tokens:\n",
    "        # Handle punctuation\n",
    "        if token in string.punctuation or token in punctuation_symbols:\n",
    "            local_punctuation_counter[token] += 1\n",
    "        else:\n",
    "            # Lemmatize using the cache\n",
    "            root_word = cached_lemmatize(token, lemmatizer)\n",
    "            local_word_counter[root_word] += 1\n",
    "\n",
    "            # Check for suffixes\n",
    "            for suffix in suffixes:\n",
    "                if token.endswith(suffix):\n",
    "                    local_suffix_counter[suffix] += 1\n",
    "                    break\n",
    "\n",
    "    # Update global counters with local counters using locks for thread safety\n",
    "    with counter_lock:\n",
    "        word_counter.update(local_word_counter)\n",
    "        suffix_counter.update(local_suffix_counter)\n",
    "        punctuation_counter.update(local_punctuation_counter)\n",
    "\n",
    "# Process the dataset with a progress bar and multithreading\n",
    "with ThreadPoolExecutor(max_workers=8) as executor:\n",
    "    futures = [executor.submit(process_document, example) for example in dataset['text']]\n",
    "    for _ in tqdm(as_completed(futures), total=len(futures), desc=\"Processing dataset\", unit=\"document\"):\n",
    "        pass\n",
    "\n",
    "# Combine all counters into one for Huffman tree building\n",
    "combined_counter = word_counter + suffix_counter + punctuation_counter\n",
    "\n",
    "# Function to build Huffman tree and generate codes\n",
    "def build_huffman_tree(frequency):\n",
    "    heap = [[weight, [symbol, \"\"]] for symbol, weight in frequency.items()]\n",
    "    heapq.heapify(heap)\n",
    "    while len(heap) > 1:\n",
    "        lo = heapq.heappop(heap)\n",
    "        hi = heapq.heappop(heap)\n",
    "        for pair in lo[1:]:\n",
    "            pair[1] = '0' + pair[1]\n",
    "        for pair in hi[1:]:\n",
    "            pair[1] = '1' + pair[1]\n",
    "        heapq.heappush(heap, [lo[0] + hi[0]] + lo[1:] + hi[1:])\n",
    "    return sorted(heapq.heappop(heap)[1:], key=lambda p: (len(p[-1]), p))\n",
    "\n",
    "# Generate Huffman codes for the combined frequencies\n",
    "huffman_codes = build_huffman_tree(combined_counter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e55c8f1d",
   "metadata": {},
   "source": [
    "### Check created codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33f010b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the Huffman codes\n",
    "for symbol, code in huffman_codes[:100]:\n",
    "    print(f\"{symbol}: {code}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97ce68d7",
   "metadata": {},
   "source": [
    "### Escape symbols and save codes to txt file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c27878c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to escape special characters for Arduino output\n",
    "def escape_for_arduino(char):\n",
    "    if char == \"'\":\n",
    "        return \"\\\\'\"  # Escape single quote\n",
    "    elif char == \"\\\"\":\n",
    "        return \"\\\\\\\"\"  # Escape double quote\n",
    "    elif char == \"\\\\\":\n",
    "        return \"\\\\\\\\\"  # Escape backslash\n",
    "    elif char == \"\\n\":\n",
    "        return \"\\\\n\"  # Escape newline\n",
    "    elif char == \"\\r\":\n",
    "        return \"\\\\r\"  # Escape carriage return\n",
    "    elif char == \"\\t\":\n",
    "        return \"\\\\t\"  # Escape tab\n",
    "    else:\n",
    "        return char  # Return other characters unchanged\n",
    "\n",
    "# Open a text file for writing the output\n",
    "with open(\"huffman_codes_output.txt\", \"w\") as file:\n",
    "    # Step 5: Write Huffman table in Arduino format\n",
    "    file.write(\"const HuffmanCode huffmanTable[] = {\\n\")\n",
    "\n",
    "    for symbol, code in huffman_codes.items():\n",
    "        # Escape special characters\n",
    "        escaped_symbol = ''.join(escape_for_arduino(char) for char in symbol)\n",
    "        # Write the entry to the file\n",
    "        file.write(f\"  {{\\\"{escaped_symbol}\\\", \\\"{code}\\\"}},\\n\")\n",
    "    \n",
    "    file.write(\"};\\n\")\n",
    "\n",
    "# Confirmation message\n",
    "print(\"Huffman codes have been saved to 'huffman_codes_output.txt'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55cfc5f9",
   "metadata": {},
   "source": [
    "### Clean Huffman Codes\n",
    "##### Clean tokens and symbols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a240a8cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load Huffman codes from a text file and remove tokens containing symbols\n",
    "def load_and_clean_huffman_codes(file_path):\n",
    "    # Define the set of allowed characters (alphanumeric only)\n",
    "    allowed_characters = re.compile(r'^[a-zA-Z0-9]+$')\n",
    "\n",
    "    cleaned_huffman_codes = {}\n",
    "\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            if \"{\" in line and \"}\" in line:\n",
    "                # Extract the key and value from the line\n",
    "                key_value = line.strip()[1:-1].split(', ')\n",
    "                key = key_value[0][1:-1]  # remove the surrounding quotes from the key\n",
    "                value = key_value[1][1:-1]  # remove the surrounding quotes from the value\n",
    "\n",
    "                # Check if the token contains only alphanumeric characters\n",
    "                cleaned_key = key.replace('\\\\\\'', '\\'').replace('\\\\\"', '\"').replace('\\\\\\\\', '\\\\')\n",
    "                if allowed_characters.match(cleaned_key):\n",
    "                    # Store the cleaned key and value in the dictionary if it passes the check\n",
    "                    cleaned_huffman_codes[cleaned_key] = value.strip('\"')\n",
    "    \n",
    "    return cleaned_huffman_codes\n",
    "\n",
    "# Specify the file path to the Huffman codes file\n",
    "huffman_codes_file = \"huffman_codes_output.txt\"\n",
    "\n",
    "# Load and clean the Huffman codes\n",
    "cleaned_huffman_codes = load_and_clean_huffman_codes(huffman_codes_file)\n",
    "\n",
    "# Optionally, save the cleaned Huffman codes back to a file\n",
    "with open(\"cleaned_huffman_codes_output.txt\", \"w\") as file:\n",
    "    for key, value in cleaned_huffman_codes.items():\n",
    "        file.write(f'{{\"{key}\", \"{value}\"}},\\n')\n",
    "\n",
    "print(\"Cleaned Huffman codes have been saved to 'cleaned_huffman_codes_output.txt'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fd0b994",
   "metadata": {},
   "source": [
    "### Test the Huffman Encoding on random phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b357f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Function to load Huffman codes from a text file\n",
    "def load_huffman_codes_from_file(file_path):\n",
    "    huffman_codes = {}\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            if \"{\" in line and \"}\" in line:\n",
    "                key_value = line.strip()[1:-1].split(', ')\n",
    "                key = key_value[0][1:-1]  # remove the surrounding quotes from the key\n",
    "                value = key_value[1][1:-1]  # remove the surrounding quotes from the value\n",
    "                \n",
    "                # Remove any extra characters that may have been escaped in the file\n",
    "                key = key.replace('\\\\\\'', '\\'').replace('\\\\\"', '\"').replace('\\\\\\\\', '\\\\')\n",
    "                value = value.replace('\\\\\\'', '\\'').replace('\\\\\"', '\"').replace('\\\\\\\\', '\\\\')\n",
    "\n",
    "                # Store the cleaned key and value in the dictionary\n",
    "                huffman_codes[key] = value.strip('\"')  # Ensure the value does not contain any quotes\n",
    "    return huffman_codes\n",
    "\n",
    "# Function to preprocess and clean the phrase, keeping ASCII characters and symbols\n",
    "def preprocess_phrase(phrase):\n",
    "    # Convert to lowercase\n",
    "    phrase = phrase.lower()\n",
    "    \n",
    "    # Add spaces around punctuation and symbols\n",
    "    phrase = re.sub(r'([.,!?;:(){}[\\]\"\\'#])', r' \\1 ', phrase)\n",
    "    \n",
    "    # Remove non-ASCII characters\n",
    "    phrase = re.sub(r'[^\\x00-\\x7F]', '', phrase)\n",
    "    \n",
    "    # Remove extra spaces\n",
    "    phrase = re.sub(r'\\s+', ' ', phrase).strip()\n",
    "    \n",
    "    return phrase\n",
    "\n",
    "# Function to lemmatize and split words into root and suffix\n",
    "def lemmatize_and_split(word):\n",
    "    if word.endswith(\"'s\"):\n",
    "        root_word = lemmatizer.lemmatize(word[:-2])\n",
    "        return root_word, \"'s\"\n",
    "    else:\n",
    "        root_word = lemmatizer.lemmatize(word)\n",
    "        suffix = word[len(root_word):] if len(word) > len(root_word) else ''\n",
    "        return root_word, suffix\n",
    "\n",
    "# Function to encode a phrase using Huffman codes and return an array of codes\n",
    "def encode_phrase(phrase, huffman_codes):\n",
    "    encoded = []\n",
    "    tokens = phrase.split()\n",
    "    for token in tokens:\n",
    "        root_word, suffix = lemmatize_and_split(token)\n",
    "        \n",
    "        if root_word in huffman_codes:\n",
    "            encoded.append(huffman_codes[root_word])\n",
    "        else:\n",
    "            raise ValueError(f\"Root word '{root_word}' not found in Huffman codes.\")\n",
    "        \n",
    "        if suffix and suffix in huffman_codes:\n",
    "            encoded.append(huffman_codes[suffix])\n",
    "        elif suffix:\n",
    "            raise ValueError(f\"Suffix '{suffix}' not found in Huffman codes.\")\n",
    "    \n",
    "    return encoded\n",
    "\n",
    "# Function to decode an array of encoded strings using Huffman codes\n",
    "def decode_phrase(encoded, huffman_codes):\n",
    "    inverse_huffman_codes = {v: k for k, v in huffman_codes.items()}\n",
    "    decoded = []\n",
    "    for code in encoded:\n",
    "        if code in inverse_huffman_codes:\n",
    "            decoded.append(inverse_huffman_codes[code])\n",
    "        else:\n",
    "            raise ValueError(f\"Code '{code}' not found in Huffman codes.\")\n",
    "    \n",
    "    # Join decoded parts and remove spaces around punctuation\n",
    "    decoded_phrase = ' '.join(decoded)\n",
    "    decoded_phrase = re.sub(r'\\s+([.,!?;:(){}[\\]\"\\'#])', r'\\1', decoded_phrase)\n",
    "    return decoded_phrase.strip()\n",
    "\n",
    "# Function to pack bit strings into a byte array\n",
    "def pack_bits(bit_strings):\n",
    "    packed_bytes = []\n",
    "    current_byte = 0\n",
    "    bits_filled = 0\n",
    "\n",
    "    for bit_string in bit_strings:\n",
    "        for bit in bit_string:\n",
    "            current_byte = (current_byte << 1) | int(bit)\n",
    "            bits_filled += 1\n",
    "            if bits_filled == 8:\n",
    "                packed_bytes.append(current_byte)\n",
    "                current_byte = 0\n",
    "                bits_filled = 0\n",
    "\n",
    "    if bits_filled > 0:\n",
    "        packed_bytes.append(current_byte << (8 - bits_filled))\n",
    "\n",
    "    return packed_bytes\n",
    "\n",
    "# Function to unpack a byte array back into a bit string\n",
    "def unpack_bits(packed_bytes, total_bits):\n",
    "    current_string = \"\"\n",
    "\n",
    "    for byte in packed_bytes:\n",
    "        for i in range(7, -1, -1):\n",
    "            bit = (byte >> i) & 1\n",
    "            current_string += str(bit)\n",
    "            total_bits -= 1\n",
    "            if total_bits == 0:\n",
    "                break\n",
    "        if total_bits == 0:\n",
    "            break\n",
    "\n",
    "    return current_string\n",
    "\n",
    "# Choose which Huffman codes to use (from file or in-memory variable)\n",
    "use_file = True  # Set to True to use Huffman codes from file\n",
    "\n",
    "if use_file:\n",
    "    huffman_codes_file = \"huffman_codes_output.txt\"\n",
    "    huffman_codes = load_huffman_codes_from_file(huffman_codes_file)\n",
    "else:\n",
    "    huffman_codes = huffman_codes  # If you have in-memory codes\n",
    "\n",
    "# Example usage\n",
    "phrase_to_encode = \"\"\"\n",
    "        Most rockets can be launched from the ground because exhaust thrust \n",
    "        from the engine is bigger than the weight of the vehicle on Earth. \n",
    "        Some are used to bring satellites into orbit, for example from a spaceport.\n",
    "        Some rockets such as ion thrusters are too wering them to outer space.\n",
    "\"\"\"\n",
    "\n",
    "# Preprocess the phrase\n",
    "processed_phrase = preprocess_phrase(phrase_to_encode)\n",
    "print(f\"Original Phrase: {phrase_to_encode}\")\n",
    "print(f\"Processed Phrase: {processed_phrase}\")\n",
    "\n",
    "# Encode the cleaned phrase\n",
    "encoded_phrase = encode_phrase(processed_phrase, huffman_codes)\n",
    "print(f\"Encoded Phrase: {encoded_phrase}\")\n",
    "\n",
    "# Pack the encoded phrase into a byte array\n",
    "packed_encoding = pack_bits(encoded_phrase)\n",
    "print(f\"Packed Encoding: {packed_encoding}\")\n",
    "\n",
    "# Unpack the byte array back into a bit string\n",
    "unpacked_encoding = unpack_bits(packed_encoding, sum(len(b) for b in encoded_phrase))\n",
    "print(f\"Unpacked Encoding: {unpacked_encoding}\")\n",
    "\n",
    "# Properly decode the phrase by extracting the correct bit lengths\n",
    "bit_index = 0\n",
    "decoded_segments = []\n",
    "for token in processed_phrase.split():\n",
    "    root_word, suffix = lemmatize_and_split(token)\n",
    "    if root_word in huffman_codes:\n",
    "        bit_length = len(huffman_codes[root_word])\n",
    "        decoded_segments.append(unpacked_encoding[bit_index:bit_index + bit_length])\n",
    "        bit_index += bit_length\n",
    "    \n",
    "    if suffix in huffman_codes:\n",
    "        bit_length = len(huffman_codes[suffix])\n",
    "        decoded_segments.append(unpacked_encoding[bit_index:bit_index + bit_length])\n",
    "        bit_index += bit_length\n",
    "\n",
    "# Decode the unpacked bit strings back into the original phrase\n",
    "decoded_phrase = decode_phrase(decoded_segments, huffman_codes)\n",
    "print(f\"Decoded Phrase: {decoded_phrase}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11 (tf-gpu-m1)",
   "language": "python",
   "name": "tf-gpu-m1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
