{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "df7b1aef",
   "metadata": {},
   "source": [
    "### Jupyter Notebook Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2e34b5b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ry/454yhlzx6hd15j7rjv4th0lw0000gn/T/ipykernel_6972/2003764878.py:1: DeprecationWarning: Importing display from IPython.core.display is deprecated since IPython 7.14, please import from IPython display\n",
      "  from IPython.core.display import display, HTML\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML                                    \n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))  \n",
    "import IPython.display as display"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12f253c4",
   "metadata": {},
   "source": [
    "### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd2699ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import heapq\n",
    "import struct\n",
    "from tqdm import tqdm\n",
    "from collections import Counter, defaultdict\n",
    "from threading import Lock\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da6a7408",
   "metadata": {},
   "source": [
    "### Radix Tree Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43907a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RadixNode and RadixTree classes as defined before\n",
    "class RadixNode:\n",
    "    def __init__(self):\n",
    "        self.children = {}\n",
    "        self.tokenID = -1\n",
    "\n",
    "class RadixTree:\n",
    "    def __init__(self):\n",
    "        self.root = RadixNode()\n",
    "\n",
    "    def insert(self, word, tokenID):\n",
    "        node = self.root\n",
    "        while word:\n",
    "            for prefix in node.children:\n",
    "                common_length = 0\n",
    "                for i in range(min(len(prefix), len(word))):\n",
    "                    if prefix[i] != word[i]:\n",
    "                        break\n",
    "                    common_length += 1\n",
    "\n",
    "                if common_length > 0:\n",
    "                    if common_length < len(prefix):\n",
    "                        existing_child = node.children.pop(prefix)\n",
    "                        new_node = RadixNode()\n",
    "                        new_node.children[prefix[common_length:]] = existing_child\n",
    "                        node.children[prefix[:common_length]] = new_node\n",
    "                        node = new_node\n",
    "                    else:\n",
    "                        node = node.children[prefix]\n",
    "\n",
    "                    word = word[common_length:]\n",
    "                    break\n",
    "            else:\n",
    "                node.children[word] = RadixNode()\n",
    "                node = node.children[word]\n",
    "                word = \"\"\n",
    "        node.tokenID = tokenID\n",
    "        \n",
    "    def traverse(self, word):\n",
    "        node = self.root\n",
    "        while word:\n",
    "            found = False\n",
    "            for prefix, child in node.children.items():\n",
    "                if word.startswith(prefix):\n",
    "                    word = word[len(prefix):]\n",
    "                    node = child\n",
    "                    found = True\n",
    "                    break\n",
    "            if not found:\n",
    "                return None\n",
    "        return node.tokenID if node.tokenID != -1 else None\n",
    "\n",
    "    def serialize_to_binary(self):\n",
    "        def node_to_binary(node):\n",
    "            children_data = b''.join(\n",
    "                struct.pack(f'i{len(prefix)}s', len(prefix), prefix.encode('utf-8')) + node_to_binary(child)\n",
    "                for prefix, child in node.children.items()\n",
    "            )\n",
    "            tokenID_data = struct.pack('i', node.tokenID)\n",
    "            num_children = len(node.children)\n",
    "            num_children_data = struct.pack('i', num_children)\n",
    "            return tokenID_data + num_children_data + children_data\n",
    "\n",
    "        return node_to_binary(self.root)\n",
    "\n",
    "    def save_to_file(self, filename):\n",
    "        binary_data = self.serialize_to_binary()\n",
    "        with open(filename, 'wb') as file:\n",
    "            file.write(binary_data)\n",
    "\n",
    "    def deserialize_from_binary(self, binary_data):\n",
    "        def binary_to_node(data, offset=0):\n",
    "            tokenID = struct.unpack_from('i', data, offset)[0]\n",
    "            offset += 4\n",
    "            num_children = struct.unpack_from('i', data, offset)[0]\n",
    "            offset += 4\n",
    "            node = RadixNode()\n",
    "            node.tokenID = tokenID\n",
    "            for _ in range(num_children):\n",
    "                prefix_len = struct.unpack_from('i', data, offset)[0]\n",
    "                offset += 4\n",
    "                prefix = struct.unpack_from(f'{prefix_len}s', data, offset)[0].decode('utf-8')\n",
    "                offset += prefix_len\n",
    "                child, offset = binary_to_node(data, offset)\n",
    "                node.children[prefix] = child\n",
    "            return node, offset\n",
    "\n",
    "        self.root, _ = binary_to_node(binary_data)\n",
    "        \n",
    "    def get_word_from_id(self, token_id):\n",
    "        return find_word_by_node_id(self.root, token_id)\n",
    "\n",
    "    def load_from_file(self, filename):\n",
    "        with open(filename, 'rb') as file:\n",
    "            binary_data = file.read()\n",
    "        self.deserialize_from_binary(binary_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c7b2dc1",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b93073",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to escape special characters in a string for C++ string literals\n",
    "def escape_special_characters(token):\n",
    "    token = token.replace('\\\\', '\\\\\\\\')\n",
    "    token = token.replace('\\\"', '\\\\\\\"')\n",
    "    token = token.replace('\\'', '\\\\\\'')\n",
    "    token = token.replace('\\n', '\\\\n')\n",
    "    token = token.replace('\\r', '\\\\r')\n",
    "    token = token.replace('\\t', '\\\\t')\n",
    "    token = token.replace('<space>', ' ')  # Convert <space> to whitespace\n",
    "    return token\n",
    "\n",
    "# Function to clean the input phrase\n",
    "def clean_input(phrase):\n",
    "    # Remove any spaces at the beginning or end\n",
    "    phrase = phrase.strip()\n",
    "    # Convert to lowercase\n",
    "    phrase = phrase.lower()\n",
    "    # Remove non-ASCII characters\n",
    "    phrase = ''.join(char for char in phrase if ord(char) < 128)\n",
    "    return phrase\n",
    "\n",
    "# Function to clean the word, removing `_` and `</w>` if applicable\n",
    "def clean_word(word):\n",
    "    if word != '_':  # Keep standalone underscore\n",
    "        word = word.lstrip('_')  # Remove leading underscore\n",
    "    word = word.replace('</w>', '')  # Remove end-of-word marker\n",
    "    word = escape_special_characters(word)\n",
    "    return word\n",
    "\n",
    "def clean_decoded_phrase(decoded_phrase):\n",
    "    words = decoded_phrase.split()\n",
    "    cleaned_words = []\n",
    "    i = 0\n",
    "    while i < len(words):\n",
    "        word = words[i]\n",
    "        \n",
    "        if word.startswith('_') and word.endswith('</w>'):\n",
    "            word = word[1:-4]  # Remove _ and </w>\n",
    "        elif word.startswith('_'):\n",
    "            word = word[1:]  # Just remove the _\n",
    "        elif word.endswith('</w>'):\n",
    "            word = word[:-4]  # Remove just </w>\n",
    "        \n",
    "        # Check if the current word is followed by a symbol\n",
    "        if i < len(words) - 1 and words[i + 1] in ['!', '?', '.', ',', ':', ';']:\n",
    "            word += words[i + 1]  # Attach the symbol directly to the word\n",
    "            i += 1  # Skip the next word as it's already merged\n",
    "        \n",
    "        cleaned_words.append(word)\n",
    "        i += 1\n",
    "    \n",
    "    return ' '.join(cleaned_words)\n",
    "\n",
    "# Function to split the phrase into words and symbols\n",
    "def tokenize_phrase(phrase):\n",
    "    # Split the phrase into words and standalone symbols\n",
    "    tokens = re.findall(r'\\w+|[^\\w\\s]', phrase, re.UNICODE)\n",
    "    return tokens\n",
    "\n",
    "# Function to load the BPE vocabulary and insert it into the Radix Tree\n",
    "def load_and_insert_vocab_into_tree(filename, radix_tree):\n",
    "    with open(filename, 'r') as file:\n",
    "        vocab = file.read().splitlines()\n",
    "\n",
    "    for idx, word in enumerate(vocab):\n",
    "        cleaned_word = clean_word(word)\n",
    "        radix_tree.insert(cleaned_word, idx)\n",
    "\n",
    "# Function to encode node IDs to binary and hex\n",
    "def encode_node_ids(node_ids):\n",
    "    # Encode to binary\n",
    "    binary_representation = b''.join(struct.pack('i', node_id) for node_id in node_ids)\n",
    "    # Encode to hex\n",
    "    hex_representation = binary_representation.hex()\n",
    "    return binary_representation, hex_representation\n",
    "        \n",
    "# Function to decode a phrase using the new system\n",
    "def phrase_to_node_ids(phrase, radix_tree):\n",
    "    tokens = tokenize_phrase(phrase)\n",
    "    node_ids = []\n",
    "    for token in tokens:\n",
    "        node_id = radix_tree.traverse(escape_special_characters(token))\n",
    "        if node_id is not None:\n",
    "            node_ids.append(node_id)\n",
    "        else:\n",
    "            raise ValueError(f\"Token '{token}' not found in the Radix Tree.\")\n",
    "    return node_ids\n",
    "\n",
    "def node_ids_to_phrase(node_ids, radix_tree_deserialized):\n",
    "    decoded_words = []\n",
    "    for node_id in node_ids:\n",
    "        word = radix_tree_deserialized.get_word_from_id(node_id)\n",
    "        \n",
    "        # Check if the word is a symbol or punctuation\n",
    "        if word.isalnum():  # If it's alphanumeric, it might need the _ or </w>\n",
    "            if not word.startswith('_'):\n",
    "                word = f\"_{word}\"\n",
    "            word += \"</w>\"\n",
    "        decoded_words.append(word)\n",
    "    \n",
    "    return ' '.join(decoded_words)\n",
    "\n",
    "# Helper function to find the word by node ID\n",
    "def find_word_by_node_id(node, target_id, prefix=''):\n",
    "    if node.tokenID == target_id:\n",
    "        return prefix\n",
    "    for child_prefix, child_node in node.children.items():\n",
    "        result = find_word_by_node_id(child_node, target_id, prefix + child_prefix)\n",
    "        if result:\n",
    "            return result\n",
    "    return None\n",
    "\n",
    "def find_children_by_node_id(node, target_id):\n",
    "    if node.tokenID == target_id:\n",
    "        return node.children\n",
    "    for child_node in node.children.values():\n",
    "        children = find_children_by_node_id(child_node, target_id)\n",
    "        if children is not None:\n",
    "            return children\n",
    "    return None\n",
    "\n",
    "def test_radix_tree(radix_tree, test_words):\n",
    "    results = {}\n",
    "    for word in test_words:\n",
    "        cleaned_word = escape_special_characters(word)\n",
    "        node_id = radix_tree.traverse(cleaned_word)\n",
    "        results[word] = {\n",
    "            \"found\": node_id is not None,\n",
    "            \"node_id\": node_id\n",
    "        }\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d178cf64",
   "metadata": {},
   "source": [
    "### Conver BPE Vocabulary to a Binary Radix Tree\n",
    "##### It will apply the special characters escaping function to have it working properly in C++\n",
    "##### This function will keep the original BPE \"_\" marker for prefix and \"</w>\" for end of word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14915d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "bpe_vocab_filepath = '/Users/ciprian/Desktop/Projects/Microcontroller Tokenizer/microcontroller-tokenizer/data/bpe-vocab_bookcorpus-small_25000-merges_cleaned-v2.txt'\n",
    "radix_tree_filepath = '/Users/ciprian/Desktop/Projects/Microcontroller Tokenizer/microcontroller-tokenizer/data/bpe-vocab_bookcorpus-small_25000-merges_cleaned-v2_radix-tree_no-markers.bin'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c49744f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the BPE vocabulary and create the Radix Tree\n",
    "radix_tree = RadixTree()\n",
    "\n",
    "load_and_insert_vocab_into_tree(bpe_vocab_filepath, radix_tree)\n",
    "\n",
    "# Save the Radix tree to a .bin file\n",
    "radix_tree.save_to_file(radix_tree_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39a27b9b",
   "metadata": {},
   "source": [
    "### Visaulise Radix Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c45567ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_radix_tree(node, prefix='', depth=0, max_depth=3):\n",
    "    \"\"\"\n",
    "    Visualizes part of the Radix Tree.\n",
    "    \n",
    "    Args:\n",
    "    - node: The current RadixNode to visualize.\n",
    "    - prefix: The current prefix string.\n",
    "    - depth: The current depth in the tree.\n",
    "    - max_depth: The maximum depth to traverse for visualization.\n",
    "    \"\"\"\n",
    "    if depth > max_depth:\n",
    "        return\n",
    "\n",
    "    indent = '  ' * depth\n",
    "    node_info = f\"TokenID: {node.tokenID}\" if node.tokenID != -1 else \"No TokenID\"\n",
    "    print(f\"{indent}{prefix} ({node_info})\")\n",
    "\n",
    "    for child_prefix, child_node in node.children.items():\n",
    "        visualize_radix_tree(child_node, prefix + child_prefix, depth + 1, max_depth)\n",
    "\n",
    "# Example usage to visualize the first few levels of the Radix Tree\n",
    "radix_tree_deserialized = RadixTree()\n",
    "radix_tree_deserialized.load_from_file(radix_tree_filepath)\n",
    "\n",
    "# Visualize the first 3 levels of the Radix Tree starting from the root\n",
    "visualize_radix_tree(radix_tree_deserialized.root, max_depth=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4f3a980",
   "metadata": {},
   "source": [
    "### Convert the binary file to a C header (.h)\n",
    "#### This can also be achieved by using the command `xxd -i bpe_vocab_small_radix_tree.bin > bpe_vocab_small_radix_tree.h` in the terminal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c3c2bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_to_c_header(binary_file_path, header_file_path, array_name):\n",
    "    with open(binary_file_path, \"rb\") as binary_file, open(header_file_path, \"w\") as header_file:\n",
    "        binary_data = binary_file.read()\n",
    "\n",
    "        # Start writing the header file, but we won't count this part in the binary comparison\n",
    "        header_file.write(f\"#ifndef {array_name.upper()}_H\\n\")\n",
    "        header_file.write(f\"#define {array_name.upper()}_H\\n\\n\")\n",
    "        header_file.write(f\"const unsigned char {array_name}[] = {{\\n    \")\n",
    "\n",
    "        # Write the binary data as a C array\n",
    "        for i, byte in enumerate(binary_data):\n",
    "            if i > 0:\n",
    "                header_file.write(\", \")\n",
    "            header_file.write(f\"0x{byte:02x}\")\n",
    "\n",
    "# Regenerate the C header file\n",
    "binary_file_path = \"simplified_radix_tree.bin\"\n",
    "header_file_path = \"simplified_radix_tree.h\"\n",
    "array_name = \"simplified_radix_tree\"\n",
    "\n",
    "binary_to_c_header(binary_file_path, header_file_path, array_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5beed76",
   "metadata": {},
   "source": [
    "### Testing the Binary Tree to find given words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc8dcf9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Radix tree from the .bin file\n",
    "radix_tree_deserialized = RadixTree()\n",
    "radix_tree_deserialized.load_from_file(radix_tree_filepath)\n",
    "\n",
    "# Test with 5 words from the vocab\n",
    "test_words = [\"rocket\", \"rocketman\", \"exasperated\", \"hello\", \"am\", \" \", \".\", \"153\"]\n",
    "results = test_radix_tree(radix_tree_deserialized, test_words)\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3a1d9ba",
   "metadata": {},
   "source": [
    "### Testing the Binary Radix Tree to encode and decode a given phrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b3529c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage of the new system\n",
    "radix_tree_deserialized = RadixTree()\n",
    "radix_tree_deserialized.load_from_file(radix_tree_filepath)\n",
    "\n",
    "phrase = \"  Hello There 5! How are You?  \"\n",
    "cleaned_phrase = clean_input(phrase)\n",
    "node_ids = phrase_to_node_ids(cleaned_phrase, radix_tree_deserialized)\n",
    "binary_representation, hex_representation = encode_node_ids(node_ids)\n",
    "decoded_phrase = node_ids_to_phrase(node_ids, radix_tree_deserialized)\n",
    "cleaned_decoded_phrase = clean_decoded_phrase(decoded_phrase)\n",
    "\n",
    "print(f\"Original Input: {phrase}\")\n",
    "print(f\"Cleaned Input: {cleaned_phrase}\")\n",
    "print(f\"Node IDs: {node_ids}\")\n",
    "print(f\"Hex Encoding: {hex_representation}\")\n",
    "print(f\"Binary Encoding: {binary_representation}\")\n",
    "print(f\"Decoded Words: {decoded_phrase}\")\n",
    "print(f\"Cleaned Decoded Phrase: {cleaned_decoded_phrase}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11 (tf-gpu-m1)",
   "language": "python",
   "name": "tf-gpu-m1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
